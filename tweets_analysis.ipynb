{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "k4ig6YjDRKuy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "from textblob import TextBlob\n",
        "\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import HashingTF, Tokenizer, StopWordsRemover"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pyspark"
      ],
      "metadata": {
        "id": "tqfgWbhBonf7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"Catch_tweets\").getOrCreate()\n",
        "\n",
        "# Load the CSV file into a Spark DataFrame\n",
        "tweets_data = spark.read.csv(\"/content/tweets.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Define a user-defined function (UDF) for sentiment analysis using TextBlob\n",
        "def analyze_sentiment(text):\n",
        "    analysis = TextBlob(text)\n",
        "    # Classify polarity as 'positive', 'negative', or 'neutral'\n",
        "    if analysis.sentiment.polarity > 0:\n",
        "        return 'positive'\n",
        "    elif analysis.sentiment.polarity < 0:\n",
        "        return 'negative'\n",
        "    else:\n",
        "        return 'neutral'"
      ],
      "metadata": {
        "id": "3k0SSEjRYgIg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the UDF with Spark\n",
        "sentiment_udf = udf(analyze_sentiment, StringType())"
      ],
      "metadata": {
        "id": "tDVuP2VgYmjQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_data = tweets_data.withColumn(\"sentiments\", sentiment_udf(\"tweets\"))"
      ],
      "metadata": {
        "id": "Qgd23p0EZJ4h"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the resulting DataFrame\n",
        "tweets_data.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxDHqZqqZLyO",
        "outputId": "49eb4260-5c2d-445a-d754-34f16d1cdc29"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------------+--------+---------------+--------------------+----------+\n",
            "|        id|               date|    flag|       username|              tweets|sentiments|\n",
            "+----------+-------------------+--------+---------------+--------------------+----------+\n",
            "|1467810672|2009-04-06 22:19:49|NO_QUERY|  scotthamilton|is upset that he ...|   neutral|\n",
            "|1467810917|2009-04-06 22:19:53|NO_QUERY|       mattycus|@Kenichan I dived...|  positive|\n",
            "|1467811184|2009-04-06 22:19:57|NO_QUERY|        ElleCTF|my whole body fee...|  positive|\n",
            "|1467811193|2009-04-06 22:19:57|NO_QUERY|         Karoli|@nationwideclass ...|  negative|\n",
            "|1467811372|2009-04-06 22:20:00|NO_QUERY|       joy_wolf|@Kwesidei not the...|  positive|\n",
            "|1467811592|2009-04-06 22:20:03|NO_QUERY|        mybirch|         Need a hug |   neutral|\n",
            "|1467811594|2009-04-06 22:20:03|NO_QUERY|           coZZ|@LOLTrish hey  lo...|  positive|\n",
            "|1467811795|2009-04-06 22:20:05|NO_QUERY|2Hood4Hollywood|@Tatiana_K nope t...|   neutral|\n",
            "|1467812025|2009-04-06 22:20:09|NO_QUERY|        mimismo|@twittera que me ...|   neutral|\n",
            "|1467812416|2009-04-06 22:20:16|NO_QUERY| erinx3leannexo|spring break in p...|  negative|\n",
            "|1467812579|2009-04-06 22:20:17|NO_QUERY|   pardonlauren|I just re-pierced...|   neutral|\n",
            "|1467812723|2009-04-06 22:20:19|NO_QUERY|           TLeC|@caregiving I cou...|   neutral|\n",
            "|1467812771|2009-04-06 22:20:19|NO_QUERY|robrobbierobert|@octolinz16 It it...|   neutral|\n",
            "|1467812784|2009-04-06 22:20:20|NO_QUERY|    bayofwolves|@smarrison i woul...|  positive|\n",
            "|1467812799|2009-04-06 22:20:20|NO_QUERY|     HairByJess|@iamjazzyfizzle I...|   neutral|\n",
            "|1467812964|2009-04-06 22:20:22|NO_QUERY| lovesongwriter|Hollis' death sce...|   neutral|\n",
            "|1467813137|2009-04-06 22:20:25|NO_QUERY|       armotley|about to file taxes |   neutral|\n",
            "|1467813579|2009-04-06 22:20:31|NO_QUERY|     starkissed|@LettyA ahh ive a...|  positive|\n",
            "|1467813782|2009-04-06 22:20:34|NO_QUERY|      gi_gi_bee|@FakerPattyPattz ...|   neutral|\n",
            "|1467813985|2009-04-06 22:20:37|NO_QUERY|         quanvu|@alydesigns i was...|  positive|\n",
            "+----------+-------------------+--------+---------------+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "col = ['tweets','sentiments']\n",
        "data = tweets_data.select(col)\n",
        "data.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7tM0K_qw2OR",
        "outputId": "2d49e514-0e0a-43f6-a5c0-2fcdfb331f4f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------+\n",
            "|              tweets|sentiments|\n",
            "+--------------------+----------+\n",
            "|is upset that he ...|   neutral|\n",
            "|@Kenichan I dived...|  positive|\n",
            "|my whole body fee...|  positive|\n",
            "|@nationwideclass ...|  negative|\n",
            "|@Kwesidei not the...|  positive|\n",
            "|         Need a hug |   neutral|\n",
            "|@LOLTrish hey  lo...|  positive|\n",
            "|@Tatiana_K nope t...|   neutral|\n",
            "|@twittera que me ...|   neutral|\n",
            "|spring break in p...|  negative|\n",
            "|I just re-pierced...|   neutral|\n",
            "|@caregiving I cou...|   neutral|\n",
            "|@octolinz16 It it...|   neutral|\n",
            "|@smarrison i woul...|  positive|\n",
            "|@iamjazzyfizzle I...|   neutral|\n",
            "|Hollis' death sce...|   neutral|\n",
            "|about to file taxes |   neutral|\n",
            "|@LettyA ahh ive a...|  positive|\n",
            "|@FakerPattyPattz ...|   neutral|\n",
            "|@alydesigns i was...|  positive|\n",
            "+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert label into integer\n",
        "from pyspark.sql.functions import when\n",
        "\n",
        "\n",
        "sentiment_mapping = {\"positive\": 1, \"negative\": 2, \"neutral\": 0}\n",
        "\n",
        "# Create a new column 'label' using when and otherwise\n",
        "data = data.withColumn(\n",
        "    \"label\",\n",
        "    when(data[\"sentiments\"] == \"positive\", sentiment_mapping[\"positive\"])\n",
        "    .when(data[\"sentiments\"] == \"negative\", sentiment_mapping[\"negative\"])\n",
        "    .otherwise(sentiment_mapping[\"neutral\"])\n",
        ")\n"
      ],
      "metadata": {
        "id": "5aG1P717w2Yh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the resulting DataFrame\n",
        "data.select(\"tweets\",\"sentiments\", \"label\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlE1l2mww2f8",
        "outputId": "033d3d5d-a367-4144-9abf-9c907ceb4861"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------+-----+\n",
            "|              tweets|sentiments|label|\n",
            "+--------------------+----------+-----+\n",
            "|is upset that he ...|   neutral|    0|\n",
            "|@Kenichan I dived...|  positive|    1|\n",
            "|my whole body fee...|  positive|    1|\n",
            "|@nationwideclass ...|  negative|    2|\n",
            "|@Kwesidei not the...|  positive|    1|\n",
            "|         Need a hug |   neutral|    0|\n",
            "|@LOLTrish hey  lo...|  positive|    1|\n",
            "|@Tatiana_K nope t...|   neutral|    0|\n",
            "|@twittera que me ...|   neutral|    0|\n",
            "|spring break in p...|  negative|    2|\n",
            "|I just re-pierced...|   neutral|    0|\n",
            "|@caregiving I cou...|   neutral|    0|\n",
            "|@octolinz16 It it...|   neutral|    0|\n",
            "|@smarrison i woul...|  positive|    1|\n",
            "|@iamjazzyfizzle I...|   neutral|    0|\n",
            "|Hollis' death sce...|   neutral|    0|\n",
            "|about to file taxes |   neutral|    0|\n",
            "|@LettyA ahh ive a...|  positive|    1|\n",
            "|@FakerPattyPattz ...|   neutral|    0|\n",
            "|@alydesigns i was...|  positive|    1|\n",
            "+--------------------+----------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "col = ['tweets','label']\n",
        "t_data = data.select(col)\n",
        "t_data.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJge21PHARlj",
        "outputId": "fbb03731-7775-42c2-c225-7c734aaf917a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+\n",
            "|              tweets|label|\n",
            "+--------------------+-----+\n",
            "|is upset that he ...|    0|\n",
            "|@Kenichan I dived...|    1|\n",
            "|my whole body fee...|    1|\n",
            "|@nationwideclass ...|    2|\n",
            "|@Kwesidei not the...|    1|\n",
            "|         Need a hug |    0|\n",
            "|@LOLTrish hey  lo...|    1|\n",
            "|@Tatiana_K nope t...|    0|\n",
            "|@twittera que me ...|    0|\n",
            "|spring break in p...|    2|\n",
            "|I just re-pierced...|    0|\n",
            "|@caregiving I cou...|    0|\n",
            "|@octolinz16 It it...|    0|\n",
            "|@smarrison i woul...|    1|\n",
            "|@iamjazzyfizzle I...|    0|\n",
            "|Hollis' death sce...|    0|\n",
            "|about to file taxes |    0|\n",
            "|@LettyA ahh ive a...|    1|\n",
            "|@FakerPattyPattz ...|    0|\n",
            "|@alydesigns i was...|    1|\n",
            "+--------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import regexp_replace\n",
        "\n",
        "def clean_tweet_text(text):\n",
        "    # Remove URLs\n",
        "    text = regexp_replace(text, r\"http\\S+|www\\S+|https\\S+\", \"\")\n",
        "    # Remove mentions\n",
        "    text = regexp_replace(text, r\"@\\w+\", \"\")\n",
        "    # Remove special characters and numbers, keep only letters\n",
        "    text = regexp_replace(text, \"[^a-zA-Z\\s]\", \"\")\n",
        "    return text\n",
        "\n",
        "# Apply the cleaning function to the 'tweets' column\n",
        "t_data = data.withColumn(clean_tweet_text(\"tweets\"))\n",
        "\n",
        "# Show the resulting DataFrame with cleaned tweets\n",
        "t_data.select.show(truncate=False)"
      ],
      "metadata": {
        "id": "g76RMpzuPFHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Divide data into 70% for training, 30% for testing\n",
        "dividedData = t_data.randomSplit([0.7, 0.3])\n",
        "trainingData = dividedData[0] #index 0 = data training\n",
        "testingData = dividedData[1] #index 1 = data testing\n",
        "train_rows = trainingData.count()\n",
        "test_rows = testingData.count()\n",
        "print (\"Training data rows:\", train_rows, \"; Testing data rows:\", test_rows)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8iw7pJZ_taq",
        "outputId": "a877f45f-4f63-4282-98a3-f45e3992de6e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data rows: 1120077 ; Testing data rows: 479922\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Clean training data***"
      ],
      "metadata": {
        "id": "m2LLIXuPqMnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from pyspark.sql.functions import regexp_replace\n",
        "\n",
        "# def clean_tweet_text(text):\n",
        "#     # Remove URLs\n",
        "#     text = regexp_replace(text, r\"http\\S+|www\\S+|https\\S+\", \"\")\n",
        "#     # Remove mentions\n",
        "#     text = regexp_replace(text, r\"@\\w+\", \"\")\n",
        "#     # Remove special characters and numbers, keep only letters\n",
        "#     text = regexp_replace(text, \"[^a-zA-Z\\s]\", \"\")\n",
        "#     return text\n",
        "\n",
        "# # Apply the cleaning function to the 'tweets' column\n",
        "# tweets_data_cleaned = data.withColumn(\"cleaned_tweets\", clean_tweet_text(\"tweets\"))\n",
        "\n",
        "# # Show the resulting DataFrame with cleaned tweets\n",
        "# tweets_data_cleaned.select(\"tweets\", \"cleaned_tweets\").show(truncate=False)"
      ],
      "metadata": {
        "id": "LDc0-tp1WhOf"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate \"tweets\" into individual words using tokenizer\n",
        "tokenizer = Tokenizer(inputCol=\"tweets\", outputCol=\"tweetWords\")\n",
        "tokenizedTrain = tokenizer.transform(trainingData)\n",
        "tokenizedTrain.show(truncate=False, n=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylOXNsBjWhVi",
        "outputId": "4b60294f-4c65-4c27-ce0e-51b7d6aad92f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------------------------------------------------------------------------------+-----+-------------------------------------------------------------------------------------------------------------------------+\n",
            "|tweets                                                                                           |label|tweetWords                                                                                                               |\n",
            "+-------------------------------------------------------------------------------------------------+-----+-------------------------------------------------------------------------------------------------------------------------+\n",
            "|                                           exhausted                                             |2    |[, , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , exhausted]                        |\n",
            "|                                     I miss her so much already...                               |1    |[, , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , i, miss, her, so, much, already...]           |\n",
            "|                     is so sad for my APL friend.............                                    |2    |[, , , , , , , , , , , , , , , , , , , , , is, so, sad, for, my, apl, friend.............]                               |\n",
            "|               I HAVE NOOOOOOOOOO FRIENDS ON TWITTER IT MAKES ME SAD WILL SOMEONE FOLLOW ME      |2    |[, , , , , , , , , , , , , , , i, have, noooooooooo, friends, on, twitter, it, makes, me, sad, will, someone, follow, me]|\n",
            "|             i just want to hear from you. i guess that's asking too much..                      |1    |[, , , , , , , , , , , , , i, just, want, to, hear, from, you., i, guess, that's, asking, too, much..]                   |\n",
            "+-------------------------------------------------------------------------------------------------+-----+-------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing stop words (unimportant words to be features)\n",
        "swr = StopWordsRemover(inputCol=tokenizer.getOutputCol(),\n",
        "                       outputCol=\"MeaningfulWords\")\n",
        "SwRemovedTrain = swr.transform(tokenizedTrain)\n",
        "SwRemovedTrain.show(truncate=False, n=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FxbRl98Whb3",
        "outputId": "654cc874-afb6-42d9-ea69-e4ac4ab2c4ff"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------------------------------------------------------------------------------+-----+-------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------+\n",
            "|tweets                                                                                           |label|tweetWords                                                                                                               |MeaningfulWords                                                                                   |\n",
            "+-------------------------------------------------------------------------------------------------+-----+-------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------+\n",
            "|                                           exhausted                                             |2    |[, , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , exhausted]                        |[, , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , exhausted] |\n",
            "|                                     I miss her so much already...                               |1    |[, , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , i, miss, her, so, much, already...]           |[, , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , miss, much, already...]|\n",
            "|                     is so sad for my APL friend.............                                    |2    |[, , , , , , , , , , , , , , , , , , , , , is, so, sad, for, my, apl, friend.............]                               |[, , , , , , , , , , , , , , , , , , , , , sad, apl, friend.............]                         |\n",
            "|               I HAVE NOOOOOOOOOO FRIENDS ON TWITTER IT MAKES ME SAD WILL SOMEONE FOLLOW ME      |2    |[, , , , , , , , , , , , , , , i, have, noooooooooo, friends, on, twitter, it, makes, me, sad, will, someone, follow, me]|[, , , , , , , , , , , , , , , noooooooooo, friends, twitter, makes, sad, someone, follow]        |\n",
            "|             i just want to hear from you. i guess that's asking too much..                      |1    |[, , , , , , , , , , , , , i, just, want, to, hear, from, you., i, guess, that's, asking, too, much..]                   |[, , , , , , , , , , , , , want, hear, you., guess, asking, much..]                               |\n",
            "+-------------------------------------------------------------------------------------------------+-----+-------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting words feature into numerical feature withHashingTF funtion for model training\n",
        "hashTF = HashingTF(inputCol=swr.getOutputCol(), outputCol=\"features\")\n",
        "numericTrainData = hashTF.transform(SwRemovedTrain).select(\n",
        "    'label', 'MeaningfulWords', 'features')\n",
        "numericTrainData.show(truncate=False, n=3)"
      ],
      "metadata": {
        "id": "ICPWZITNBkwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5CdfvIMdZCsD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Modeling***"
      ],
      "metadata": {
        "id": "8xoy8otzq6BK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train our classifier model using training data\n",
        "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\",\n",
        "                        maxIter=10, regParam=0.01)\n",
        "model = lr.fit(numericTrainData)\n",
        "print (\"Training is done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXNLABBD2fmm",
        "outputId": "bbba4202-9d51-4a80-f242-91c89ddcce9b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training is done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testingData.show( n=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-nN8dHy8vGg",
        "outputId": "83f9f108-7bff-4cc6-830f-0fef6c53eef0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------+-----+--------------------+--------------------+\n",
            "|              tweets|sentiments|label|      cleaned_tweets|      sentimentWords|\n",
            "+--------------------+----------+-----+--------------------+--------------------+\n",
            "|                 ...|  positive|    1|                 ...|[, , , , , , , , ...|\n",
            "|            Miss ...|  positive|    1|            Miss ...|[, , , , , , , , ...|\n",
            "|         or i jus...|  positive|    1|         or i jus...|[, , , , , , , , ...|\n",
            "+--------------------+----------+-----+--------------------+--------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Prepare testing data\n",
        "tokenizedTest = tokenizer.transform(testingData)\n",
        "SwRemovedTest = swr.transform(tokenizedTest)\n",
        "numericTest = hashTF.transform(SwRemovedTest).select(\n",
        "    'Label', 'MeaningfulWords', 'features')\n",
        "numericTest.show(truncate=False, n=2)"
      ],
      "metadata": {
        "id": "IaX0RW_a2frx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4150ac80-86ce-4fe4-960e-fa62d6b71924"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---------------------------------------------------------------------+-------------------------------------------------------------------+\n",
            "|Label|MeaningfulWords                                                      |features                                                           |\n",
            "+-----+---------------------------------------------------------------------+-------------------------------------------------------------------+\n",
            "|1    |[, , , , , , , , , , , , , , , , , , , missed, new, moon, trailer...]|(262144,[89833,165360,201103,244504,249180],[1.0,1.0,1.0,1.0,19.0])|\n",
            "|0    |[, , , , , , , , , , , , , , , practising.....how, feel]             |(262144,[61899,231362,249180],[1.0,1.0,15.0])                      |\n",
            "+-----+---------------------------------------------------------------------+-------------------------------------------------------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict testing data and calculate the accuracy model\n",
        "prediction = model.transform(numericTest)\n",
        "predictionFinal = prediction.select(\n",
        "    \"MeaningfulWords\", \"prediction\", \"Label\")\n",
        "predictionFinal.show(n=4, truncate = False)\n",
        "correctPrediction = predictionFinal.filter(\n",
        "    predictionFinal['prediction'] == predictionFinal['Label']).count()\n",
        "totalData = predictionFinal.count()\n",
        "print(\"correct prediction:\", correctPrediction, \", total data:\", totalData,\n",
        "      \", accuracy:\", correctPrediction/totalData)"
      ],
      "metadata": {
        "id": "hOeVItYN2fvl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9d2701d-a25b-4f6c-e69b-72dcc8d10f0f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------------------------------------------------------------------------------------------+----------+-----+\n",
            "|MeaningfulWords                                                                                                               |prediction|Label|\n",
            "+------------------------------------------------------------------------------------------------------------------------------+----------+-----+\n",
            "|[, , , , , , , , , , , , , , , , , , , missed, new, moon, trailer...]                                                         |1.0       |1    |\n",
            "|[, , , , , , , , , , , , , , , practising.....how, feel]                                                                      |0.0       |0    |\n",
            "|[, , , , , , , , , , , , miss, love, jamie]                                                                                   |1.0       |1    |\n",
            "|[, , , , , , , , , , .., omgaga., im, sooo, , im, gunna, cry., dentist, since, 11.., suposed, 2, get, crown, put, (30mins)...]|0.0       |0    |\n",
            "+------------------------------------------------------------------------------------------------------------------------------+----------+-----+\n",
            "only showing top 4 rows\n",
            "\n",
            "correct prediction: 354144 , total data: 479922 , accuracy: 0.7379199119856977\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fahGLzck2fyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hb8uj7bf2f1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Save the DataFrame to a CSV file\n",
        "tweets_data.coalesce(1).write.csv('sentiments.csv', header=True, mode='overwrite')"
      ],
      "metadata": {
        "id": "ly8_Xc22ZONe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3 = pd.read_csv('/content/sentiments.csv/open.csv', error_bad_lines=False)"
      ],
      "metadata": {
        "id": "UPS29r5zrLlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3"
      ],
      "metadata": {
        "id": "G9ipxApsrqOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import csv\n",
        "# field = [\"id\",\"date\",\"flag\",\"username\",\"tweets\",\"sentiments\"]\n",
        "# def spark_to_csv(data, file_path):\n",
        "#     \"\"\" Converts spark dataframe to CSV file \"\"\"\n",
        "#     with open(file_path, \"w\") as f:\n",
        "#         writer = csv.DictWriter(f, fieldnames= field)\n",
        "#         writer.writerow(dict(zip(fieldnames, fieldnames)))\n",
        "#         for row in data.toLocalIterator():\n",
        "#             writer.writerow(row.asDict())"
      ],
      "metadata": {
        "id": "chBfAj5maSdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3['date'] = pd.to_datetime(df3['date'].dt.strftime('%Y-%m-%d %H:%M:%S'))"
      ],
      "metadata": {
        "id": "ScWlVxNIdxlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3"
      ],
      "metadata": {
        "id": "3NBnD4qSmnqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tMom2h-SmoX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "owAp3Ivjsory"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}